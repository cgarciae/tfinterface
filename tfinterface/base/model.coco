from .base_class import Base
from .inputs import GeneralInputs, Inputs
import tensorflow as tf
from tfinterface.decorators import return_self, with_graph_as_default, copy_self
from abc import abstractmethod, ABCMeta
import os
from tfinterface import utils
import numpy as np
ModeKeys = tf.estimator.ModeKeys

class ModelBase(Base):

    __metaclass__ = ABCMeta

    def __init__(self, name, graph=None, sess=None, model_path=None, logs_path="logs", seed=None):
        super(ModelBase, self).__init__(name, graph=graph, sess=sess)

        with self.graph.as_default():
            self.seed = seed
            self.model_path = model_path if model_path else name
            self.logs_path = logs_path

            if self.seed is not None:
                tf.set_random_seed(self.seed)


    @return_self
    @with_graph_as_default
    def initialize(self, restore=False, model_path=None, var_list = None, only_in_scope = False):
        if not restore:
            self.sess.run(tf.global_variables_initializer())
        else:
            if not var_list and only_in_scope:
                var_list = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, self.name)

            model_path = (self.model_path if not model_path else model_path) |> os.path.abspath
            tf.train.Saver(var_list = var_list).restore(self.sess, model_path)

    @return_self
    @with_graph_as_default
    def save(self, model_path=None, var_list = None, only_trainable = False):
        if only_trainable and not var_list:
            var_list = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)

        model_path = (self.model_path if not model_path else model_path) |> os.path.abspath
        utils.make_dirs_for_path(model_path)
        tf.train.Saver(var_list = var_list).save(self.sess, model_path)

    @with_graph_as_default
    def get_variables(self, graph_keys=tf.GraphKeys.TRAINABLE_VARIABLES, scope=None):
        scope = scope if scope else self.name
        return tf.get_collection(graph_keys, scope=scope)

    @with_graph_as_default
    def count_weights(self, *args, **kwargs):
        return (
            self.get_variables(*args, **kwargs)
            |> map$(.get_shape())
            |> map$(.as_list())
            |> map$(np.prod)
            |> list
            |> np.sum
        )

    @return_self
    def build_tensors(self, *args, **kwargs):
        self.inputs = self.get_inputs(*args, **kwargs)
        super(ModelBase, self).build_tensors(*args, **kwargs)


    def get_inputs(self, inputs, *args, **kwargs):
        return inputs


    @abstractmethod
    def predict(self, *args, **kwargs):
        pass

    def run(self, tensors, **kwargs):

        if hasattr(tensors, "__iter__"):
            tensors = [ getattr(self, tensor) for tensor in tensors ]
        elif isinstance(tensors, dict):
            tensors = { key: getattr(self, tensor) for key, tensor in tensors.items() }
        else:
            tensors = getattr(self, tensors)

        feed = self.inputs.predict_feed(**kwargs)

        return self.sess.run(tensors, feed_dict = feed)


    def batch_predict(self, generator, print_fn=None, **kwargs):

        preds_list = []

        for batch in generator:
            kwargs = kwargs.copy()
            kwargs.update(batch)

            preds = self.predict(**kwargs)
            preds_list.append(preds)

            if print_fn:
                print_fn(batch)

        return preds_list

    @with_graph_as_default
    @copy_self
    def __call__(self, *args, **kwargs):
        self._mode = kwargs.pop("mode", ModeKeys.TRAIN)
        return super(ModelBase, self).__call__(*args, **kwargs)


class Model(ModelBase):

    

    def predict(self, *args, **kwargs):
        pass
